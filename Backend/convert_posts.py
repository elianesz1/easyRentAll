import firebase_admin
from firebase_admin import credentials, firestore
import openai
import json
import time
import re
from datetime import datetime
import os
from dotenv import load_dotenv
import hashlib

print("ğŸ” Running NEW version of convert_posts.py")

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")


# === Firebase Initialization ===
cred = credentials.Certificate("serviceAccountKey.json")
firebase_admin.initialize_app(cred)
db = firestore.client()

# Function to clean the post text by removing time-related phrases
# This is to ensure that the text is more focused on the apartment details
def clean_post_text(raw_text):
    import re

    # Remove special characters that look strange
    raw_text = raw_text.strip().replace('\u200f', '').replace('\xa0', ' ')

    # Prepare patterns that usually appear at the beginning of a Facebook post
    # For example: "Shared with group", or "2 days", "5 hours", etc.
    time_pattern = r"\d+\s*(×™××™×|×©×¢×•×ª|×©× ×™×”|×©× ×™×•×ª|×“×§×•×ª)"
    start_keywords = [r"××©×•×ª×£ ×¢×: ×§×‘×•×¦×” ×¦×™×‘×•×¨×™×ª", time_pattern]

    # We will find the latest position of these patterns and remove everything before it
    min_start = 0
    for pattern in start_keywords:
        match = re.search(pattern, raw_text)
        if match:
            min_start = max(min_start, match.end()) # Keep the latest match

    # Cut the text to remove the beginning part that is not needed
    raw_text = raw_text[min_start:].lstrip(" Â·\n")

    # These are common endings in Facebook posts, like likes, shares, and group info
    end_patterns = [
        r"\+\d[\d,\.â‚ª ]* Â· .*?TA.*?×“×™×¨×” ×¢×.*?×—×“×¨×™ ×××‘×˜×™×”",
        r"×ª×œ ××‘×™×‘.*?TA.*?×“×™×¨×” ×¢×.*?×—×“×¨×™ ×××‘×˜×™×”",
        r"×”×•×“×¢×”\s*×œ×™×™×§\s*×ª×’×•×‘×”\s*×©×™×ª×•×£",
        r"×›×ª×™×‘×ª ×ª×’×•×‘×” ×¦×™×‘×•×¨×™×ª.*$"
    ]

    # Find if any of these endings are inside the post
    # If yes â€“ cut the post at that point
    for pattern in end_patterns:
        match = re.search(pattern, raw_text, re.DOTALL)
        if match:
            raw_text = raw_text[:match.start()]
            break

    return raw_text.strip() # Return the cleaned post

def generate_fingerprint(data):
    components = []

    if data.get("address"):
        components.append(f"address={data['address'].strip().lower()}")
    if data.get("rooms"):
        components.append(f"rooms={data['rooms']}")
    if data.get("price"):
        components.append(f"price={data['price']}")
    if data.get("available_from"):
        components.append(f"available_from={data['available_from']}")

    if not components:
        return None

    fingerprint_key = "|".join(components)
    return hashlib.md5(fingerprint_key.encode('utf-8')).hexdigest()

default_structure = {
    "title": "×“×™×¨×” ×œ××›×™×¨×”",
    "description": "",
    "price": None,
    "rooms": None,
    "size": None,
    "neighborhood": None,
    "address": None,
    "floor": None,
    "property_type": None,
    "pets_allowed": None,
    "has_broker": None,
    "has_balcony": None,
    "has_safe_room": None,
    "has_parking": None,
    "has_elevator": None,
    "available_from": None,
    "facebook_url": None,
    "category": None,
    }

# === Safe JSON parse ===
#××•×œ×™ ×¦×¨×™×š ×œ××—×•×§ ××•×ª×” × ×‘×“×•×§ ×¢×œ ×¤×•×¡×˜×™× ×—×“×©×™× (××—×§× ×• ××ª ×”×©×•×¨×” ×©×”×©×ª××©×” ×‘×¤×•× ×¦×§×™×” ×”×–×•)
def parse_gpt_output_safe(raw_text):
    try:
        print("RAW BEFORE PARSE:", repr(raw_text))

        # Step 1: Replace smart quotes and invisible characters
        raw_text = raw_text.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'").replace("`", "'")
        raw_text = re.sub(r'[\u200e\u200f\u202a\u202c\u202d\u202e\u2066\u2067\u2068\u2069]', '', raw_text)
        raw_text = re.sub(r'[\x00-\x1F]+', '', raw_text)

        # Step 2: Fix common escape sequence issues
        raw_text = raw_text.replace('\\"', '"')
        raw_text = raw_text.replace('\\\"', '"')
        raw_text = raw_text.replace('\\\\', '\\')

        # Step 3: Remove escaped currency symbols like "×©\"×—"
        raw_text = re.sub(r':\s*".*?×©\\\"×—"', lambda m: m.group(0).replace('×©\\"×—', ''), raw_text)

        # Step 4: Remove double quotes inside string values to avoid breaking the JSON
        raw_text = re.sub(
            r'(":[\s]*")([^"]*?)"([^"]*?)"([^"]*?)(")',
            lambda m: f'{m.group(1)}{m.group(2)}{m.group(3)}{m.group(4)}{m.group(5)}',
            raw_text
        )

        # Step 5: Add closing brace if missing
        if raw_text.count("{") > raw_text.count("}"):
            raw_text += "}"

        # Final attempt to parse the cleaned string as JSON
        return json.loads(raw_text)

    except Exception as e:
        print(f"âŒ Cleaned JSON still failed: {e}")
        try:
            # Try to show where the parsing failed
            error_index = int(str(e).split("char")[1].strip().strip(")"))
            print("Offending character:", repr(raw_text[error_index-20:error_index+20]))
        except Exception:
            print("Couldn't extract error location.")
        return None
 
# Function to extract apartment data from a Facebook post
# This function uses OpenAI's GPT-4o-mini model to analyze the post text and extract relevant information
def extract_apartment_data(post_text):
    current_year = datetime.now().year

    prompt = f"""
You are a data extraction assistant specializing in Israeli real estate posts on Facebook.

TASK: Extract data from this Facebook post about apartments for rent in Tel Aviv.

CRITICAL INSTRUCTIONS:
1. Most Facebook posts ARE apartment listings unless they're clearly just brief comments.
2. For neighborhood field, ONLY use standard Tel Aviv neighborhood names in English (see list below).
3. Use the CURRENT YEAR ({current_year}) for all dates unless explicitly stated otherwise.
4. For address field:
   - KEEP THE STREET NAME IN HEBREW even if there is no building number
   - Extract only the basic street information (name and number if available)
   - REMOVE marketing/descriptive terms like: '×™×•×§×¨×ª×™', '××“×”×™×', '××•×©×œ×', '××”×××ª', '× ×“×™×¨', '××™×•×—×“', '× ×”×“×¨', '××˜×•×¤×—', etc.
   - Extract locations even when preceded by '×‘×¨×—×•×‘', '×‘××–×•×¨', etc.
5. If a post contains text with apartment details and comments, focus ONLY on the apartment details.
6. Notice that the parameters title, description, and address are in Hebrew.

CATEGORY CLASSIFICATION (MANDATORY):
- Return a Hebrew value in "category" with EXACTLY one of:
  - "×©×›×™×¨×•×ª"  â†’ regular rental (e.g., "×œ×”×©×›×¨×”", monthly rent, deposit)
  - "××›×™×¨×”"   â†’ for sale (e.g., "×œ××›×™×¨×”", asking price, deal/transaction)
  - "×¡××‘×œ×˜"   â†’ sublet / temporary rental (e.g., "×¡××‘×œ×˜", "×”×©×›×¨×” ×–×× ×™×ª", clear start/end dates for weeks/months)
  - "×”×—×œ×¤×”"  â†’ home exchange / swap (e.g., "×“×™×¨×” ×œ×”×—×œ×¤×”", "×”×—×œ×¤×ª ×“×™×¨×”", "××—×œ×™×¤×™× ×“×™×¨×”", "home exchange", "house swap")
- Prefer "×¡××‘×œ×˜" when the stay is clearly temporary even if "×œ×”×©×›×¨×”" appears.
- If the post is about exchanging apartments (swap) and not about price/rent/sale, choose "×”×—×œ×¤×”".
- If unclear, choose "×©×›×™×¨×•×ª" (NOT null).

For apartment listings, provide this JSON structure:
{{
  "is_apartment": true,
  "category": "<×©×›×™×¨×•×ª|××›×™×¨×”|×¡××‘×œ×˜>",
  "title": "<Hebrew title - first meaningful phrase>",
  "description": "<leave empty, we will fill it from the original post text>",
  "price": <number or null>,
  "rooms": <number or null>,
  "size": <number or null>,
  "neighborhood": "<English neighborhood name from the list below>",
  "address": "<ONLY the Hebrew street name/location, NO marketing terms>",
  "floor": <number or null>,
  "property_type": "<English: apartment, duplex, penthouse, studio, etc>",
  "pets_allowed": <boolean or null>,
  "has_broker": <boolean or null>,
  "has_balcony": <boolean or null>,
  "has_safe_room": <boolean or null>,
  "has_parking": <boolean or null>,
  "has_elevator": <boolean or null>,
  "available_from": "<YYYY-MM-DD with current year {current_year}>",
  "facebook_url": "<facebook URL or null>"
}}

If the post is a short comment, question, or response without any clear apartment details (e.g., "×›××”?", "××©××— ×œ×¤×¨×˜×™×", "××¤×©×¨ ××—×™×¨?", "× ×©××¢ ×˜×•×‘", "×©×™×ª×•×£") â€” then DO NOT attempt to extract fake apartment data.

In such cases, return ONLY:
{{"is_apartment": false}}

IMPORTANT FORMAT INSTRUCTIONS:
- Always return ONLY a valid JSON object.
- Do NOT wrap the response in ```json or any markdown.
- Ensure the JSON is always complete and includes all fields, even if their value is null or empty.
- If you cannot find a value, set it explicitly to null or an empty list.
- Make sure the JSON is valid and can be parsed correctly.
- Do not include any special characters like quotes ("), backslashes (\), or smart punctuation.
- Avoid writing any content in the "description" field â€” we will fill it ourselves from the original post.
- If a field contains currency symbols like "×©\"×—", "â‚ª", or commas in numbers â€” remove them entirely.
- Do not escape any characters. Return clean JSON with plain UTF-8 text.

STANDARD TEL AVIV NEIGHBORHOODS (Use ONLY these in English):
- Old North
- New North
- Neve Tzedek
- Florentin
- Kerem HaTeimanim
- City Center
- Ramat Aviv
- Ramat Aviv Gimel
- Ramat HaHayal
- Bavli
- Yad Eliyahu
- Neve Shaanan
- Shapira
- Kfar Shalem
- Hatikva
- Bitzaron
- Montefiore
- Ajami
- Jaffa D
- Jaffa G
- Old Jaffa
- Neve Ofer
- Tel Kabir
- Neve Avivim
- Givat Amal
- Hadar Yosef
- Neve Sharett
- Tel Baruch
- North Tel Baruch
- Ma'oz Aviv
- Neve Golan
- Neve Chen
- Ganei Tzahala
- Tzahala
- Azorei Chen
- Migdal Neve Tzedek
- Gan Meir
- Bazel

FEATURES TRANSLATION:
- ××¢×œ×™×ª â†’ elevator
- ×—× ×™×” â†’ parking
- ××¨×¤×¡×ª â†’ balcony
- ×¨×™×”×•×˜ â†’ furnished
- ××—×¡×Ÿ â†’ storage
- ×¡×•×¨×’×™× â†’ bars
- ×××´×“ â†’ safe_room
- ×“×•×“ ×©××© â†’ solar_heater
- ××™×–×•×’ â†’ air_conditioning
- ×’×’ â†’ roof
- ××¨×¤×¡×ª ×©××© â†’ sun_balcony

âš ï¸ Output MUST be valid JSON. Even one incorrect quote or unescaped character will break it.

TEXT TO ANALYZE:
{post_text}
"""

    try:
        # Send request to OpenAI to extract information from the post
        response = openai.chat.completions.create(
            model="gpt-4o-mini", # This is the AI model we use
            messages=[
                # System message tells the AI to return only clean JSON
                {"role": "system", "content": "Return ONLY a valid JSON. No explanations, no markdown."},
                # User message includes the prompt + the post text we want to analyze
                {"role": "user", "content": prompt}
            ],
            temperature=0.1, # Low temperature = more accurate, less creative
            max_tokens=3000 # Max length of the response from the model
        )
        # Get the model response and remove spaces at the start and end
        result_text = response.choices[0].message.content.strip()

        # Print the full raw response from GPT for debugging
        print(" FULL GPT OUTPUT:")
        print(result_text)

        # Try to clean and convert the result from text into a Python dictionary
        parsed_data = json.loads(result_text)

        if parsed_data is None:
            print(f"JSON parsing failed:\n{result_text[:500]}")
            return None # Stop if the result is not valid

        # Start from an empty/default apartment structure
        full_data = default_structure.copy()

        # Add the data we got from GPT to the default structure
        full_data.update(parsed_data)

        # Ensure category default if missing (robustness)
        if full_data.get("is_apartment") and not full_data.get("category"):
            full_data["category"] = "×©×›×™×¨×•×ª"

        # Check if the address includes the same name as the neighborhood (in Hebrew)
        # If yes â€“ remove the address to avoid repeating it
        if full_data.get("address") and full_data.get("neighborhood"):
            hebrew_neighborhoods = {
                "Old North": "×”×¦×¤×•×Ÿ ×”×™×©×Ÿ",
                "New North": "×”×¦×¤×•×Ÿ ×”×—×“×©",
                "Neve Tzedek": "× ×•×•×” ×¦×“×§",
                "Florentin": "×¤×œ×•×¨× ×˜×™×Ÿ",
                "Kerem HaTeimanim": "×›×¨× ×”×ª×™×× ×™×",
                "City Center": "×œ×‘ ×ª×œ ××‘×™×‘",
                "Ramat Aviv": "×¨××ª ××‘×™×‘",
                "Ramat Aviv Gimel": "×¨××ª ××‘×™×‘ ×’'",
                "Ramat HaHayal": "×¨××ª ×”×—×™×™×œ",
                "Bavli": "×‘×‘×œ×™",
                "Yad Eliyahu": "×™×“ ××œ×™×”×•",
                "Neve Shaanan": "× ×•×•×” ×©×× ×Ÿ",
                "Shapira": "×©×¤×™×¨×",
                "Kfar Shalem": "×›×¤×¨ ×©×œ×",
                "Hatikva": "×”×ª×§×•×•×”",
                "Bitzaron": "×‘×™×¦×¨×•×Ÿ",
                "Montefiore": "××•× ×˜×™×¤×™×•×¨×™",
                "Ajami": "×¢×’'××™",
                "Jaffa D": "×™×¤×• ×“'",
                "Jaffa G": "×™×¤×• ×’'",
                "Old Jaffa": "×™×¤×• ×”×¢×ª×™×§×”",
                "Neve Ofer": "× ×•×•×” ×¢×•×¤×¨",
                "Tel Kabir": "×ª×œ ×›×‘×™×¨",
                "Neve Avivim": "× ×•×•×” ××‘×™×‘×™×",
                "Givat Amal": "×’×‘×¢×ª ×¢××œ",
                "Hadar Yosef": "×”×“×¨ ×™×•×¡×£",
                "Neve Sharett": "× ×•×•×” ×©×¨×ª",
                "Tel Baruch": "×ª×œ ×‘×¨×•×š",
                "North Tel Baruch": "×ª×œ ×‘×¨×•×š ×¦×¤×•×Ÿ",
                "Ma'oz Aviv": "××¢×•×– ××‘×™×‘",
                "Neve Golan": "× ×•×•×” ×’×•×œ×Ÿ",
                "Neve Chen": "× ×•×•×” ×—×Ÿ",
                "Ganei Tzahala": "×’× ×™ ×¦×”×œ×”",
                "Tzahala": "×¦×”×œ×”",
                "Azorei Chen": "××–×•×¨×™ ×—×Ÿ",
                "Migdal Neve Tzedek": "××’×“×œ × ×•×•×” ×¦×“×§",
                "Gan Meir": "×’×Ÿ ×××™×¨",
                "Bazel": "×‘×–×œ"
            }
            # If address contains the Hebrew name of the neighborhood, we remove it
            hebrew_neighborhood = hebrew_neighborhoods.get(full_data["neighborhood"])
            if hebrew_neighborhood and hebrew_neighborhood in full_data["address"]:
                full_data["address"] = None

        return full_data # Return the final clean apartment data

    except Exception as e:
        # If something goes wrong (like internet or API error), print the error
        print(f"API Error: {e}")
        return None
    
# === Main process: fetch, extract and upload ===
# Get all posts from the "posts" collection where status is "new" or "error"
posts_ref = db.collection("posts")
new_posts = posts_ref.where("status", "in", ["new", "error"]).stream()

# Counter to keep track of how many posts were saved
processed = 0

# Loop over each post from the database
for doc in new_posts:
    post = doc.to_dict() # Convert Firestore document to a dictionary
    post_id = post["id"] # Get the post ID
    post_text = post.get("text", "").strip() # Get the post text, default to empty string if not found

    print(f"\n Processing post {post_id}...") # Print which post we are working on

    # If the post text is empty, skip it
    if not post_text:
        print("Skipping empty post.")
        continue

    # If the post text is too short and contains common comment phrases, skip it
    if len(post_text) < 50 and re.search(r"(×›××”|××—×™×¨|×¤×¨×˜×™×|××©××—|××¤×©×¨|×œ××”|× ×©××¢|××¢× ×™×™×Ÿ|×©×™×ª×•×£|\?)", post_text):
        print("Skipping likely comment.") # It's probably not a real apartment post
        posts_ref.document(post_id).update({"status": "skipped"}) # Mark it as skipped
        continue
    
    # Try to extract apartment data using GPT
    data = extract_apartment_data(post_text)

    # If GPT could not return valid data â€“ mark it as error and save the post to error log
    if data is None:
        print("Skipping post due to parsing failure.")
        posts_ref.document(post_id).update({"status": "error"}) # Mark post as error

        # Save the bad post to a local file so we can look at it later
        with open("error_log.jsonl", "a", encoding="utf-8") as log_file:
            log_entry = {
                "id": post_id,
                "text": post_text
            }
            log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        continue

    try:
        # If GPT says this is NOT an apartment post â€“ skip it
        if data.get("is_apartment") is False:
            print("âœ— Not an apartment listing.")
            posts_ref.document(post_id).update({"status": "skipped"})
            continue
        
       # If the category is "×”×—×œ×¤×”" (home exchange) â€“ skip it
        if data.get("category") == "×”×—×œ×¤×”":
            print("âœ— Home exchange (×”×—×œ×¤×”) â€” skipping.")
            posts_ref.document(post_id).update({"status": "skipped_exchange"})
            continue

         # Remove the "is_apartment" key before saving to database
        if "is_apartment" in data:
            del data["is_apartment"]

         # Add extra info before saving
        data["id"] = post_id # Add the post ID
        data["images"] = post.get("images", []) # Add the list of images (if any)
        data["description"] = clean_post_text(post_text) # Clean and add the description

        # ×™×¦×™×¨×ª fingerprint ×œ×¤×™ ×©×“×•×ª ×§×™×™××™×
        fingerprint = generate_fingerprint(data)

        if not fingerprint:
            print(f"âœ— Could not generate fingerprint for post {post_id} â€“ skipping.")
            posts_ref.document(post_id).update({"status": "incomplete"})
            continue

        # ×‘×“×™×§×ª ×›×¤×™×œ×•×ª
        existing = db.collection("apartments").where("fingerprint", "==", fingerprint).get()
        if existing:
            print(f"âœ— Duplicate apartment (fingerprint match) â€“ skipping.")
            posts_ref.document(post_id).update({"status": "duplicate"})
            continue

        #  Add the fingerprint to the data
        data["fingerprint"] = fingerprint

        # Check if all important fields are present
        # We need at least price, address, and rooms to consider it a valid apartment post
        important_fields = ["address", "rooms", "price"]
        filled_fields = [f for f in important_fields if data.get(f)]

        if len(filled_fields) == 0:
            print(f"âœ— Skipping post {post_id} â€“ no important fields present.")
            posts_ref.document(post_id).update({"status": "incomplete"})  # ×¡×˜×˜×•×¡ ×—×“×© ×× ×ª×¨×¦×™ ×œ×¢×‘×•×¨ ×¢×œ×™×”× ×‘×¢×ª×™×“
            continue

        # Save the full apartment data to the "apartments" collection
        db.collection("apartments").document(post_id).set(data)

        # Mark the original post as processed
        posts_ref.document(post_id).update({"status": "processed"})
        processed += 1 # Count how many were saved
        print(f"Apartment saved: {post_id}")

    except Exception as e:
        # If something went wrong while saving â€“ print error and mark as error
        print(f"Error processing {post_id}: {e}")
        posts_ref.document(post_id).update({"status": "error"})

    time.sleep(0.5)  # Wait half a second before the next post (to avoid overload)

# Print how many apartments we saved at the end
print(f"\n Done! {processed} apartments saved.")